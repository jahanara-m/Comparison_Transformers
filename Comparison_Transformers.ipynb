{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. SETUP & INSTALLATION"
      ],
      "metadata": {
        "id": "aywPzf3ZuVGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install -q transformers torch pandas scikit-learn nltk seaborn matplotlib\n",
        "!pip install -q accelerate\n",
        "\n",
        "# For MentalRoBERTa authentication\n",
        "!pip install -q huggingface_hub\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "set_seed(42)\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "id": "3CPYTH6fuXUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. DATA LOADING & PREPROCESSING FUNCTIONS"
      ],
      "metadata": {
        "id": "wBuO4cH2ubuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK resources\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet, stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "import re\n",
        "\n",
        "# Configuration class\n",
        "class Config:\n",
        "    def __init__(self):\n",
        "        # Data paths\n",
        "        self.data_path = '/content/Dataset.csv'\n",
        "\n",
        "        # Preprocessing options\n",
        "        self.remove_stopwords = True\n",
        "        self.use_stemming = True\n",
        "        self.expand_contractions = True\n",
        "        self.expand_abbreviations = True\n",
        "\n",
        "        # Model parameters\n",
        "        self.model_name = 'roberta-base'  # Will be updated per experiment\n",
        "        self.num_labels = 6\n",
        "        self.max_length = 128\n",
        "\n",
        "        # Training parameters\n",
        "        self.batch_size = 8\n",
        "        self.eval_batch_size = 32\n",
        "        self.learning_rate = 1e-5\n",
        "        self.fine_tune_epochs = 2\n",
        "        self.classifier_epochs = 1\n",
        "\n",
        "        # Directories\n",
        "        self.results_dir = '/content/results'\n",
        "\n",
        "    def update_for_experiment(self, exp_name, model_name, preprocessing_type='basic'):\n",
        "        \"\"\"Update config for specific experiment\"\"\"\n",
        "        self.exp_name = exp_name\n",
        "        self.model_name = model_name\n",
        "\n",
        "        if preprocessing_type == 'advanced':\n",
        "            self.remove_stopwords = True\n",
        "            self.use_stemming = True\n",
        "        else:\n",
        "            self.remove_stopwords = False\n",
        "            self.use_stemming = False\n",
        "\n",
        "        # Create experiment directory\n",
        "        self.exp_dir = os.path.join(self.results_dir, exp_name)\n",
        "        os.makedirs(self.exp_dir, exist_ok=True)\n",
        "\n",
        "        return self\n",
        "\n",
        "# Preprocessing functions\n",
        "def get_wordnet_pos(token):\n",
        "    \"\"\"Get WordNet POS tag from NLTK POS tag\"\"\"\n",
        "    if not token:\n",
        "        return wordnet.NOUN\n",
        "    try:\n",
        "        tag = nltk.pos_tag([token])[0][1][0].upper()\n",
        "    except:\n",
        "        return wordnet.NOUN  # Default to noun if tagging fails\n",
        "    tag_dict = {\"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV, \"J\": wordnet.ADJ}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "def preprocess_tokens(tokens, config):\n",
        "    \"\"\"Apply stopword removal and stemming based on config\"\"\"\n",
        "    if config.remove_stopwords:\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "    if config.use_stemming:\n",
        "        stemmer = PorterStemmer()\n",
        "        tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def preprocess_text(text, config):\n",
        "    \"\"\"Main preprocessing function\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return ''\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "    # Tokenize and lemmatize\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]\n",
        "\n",
        "    # Apply stopword removal and stemming\n",
        "    tokens = preprocess_tokens(tokens, config)\n",
        "\n",
        "    # Expand contractions\n",
        "    if config.expand_contractions:\n",
        "        contraction_mapping = {\n",
        "            \"n't\": \" not\", \"'s\": \" is\", \"'m\": \" am\", \"'re\": \" are\", \"'ve\": \" have\", \"'ll\": \" will\"\n",
        "        }\n",
        "        tokens = [contraction_mapping.get(token, token) for token in tokens]\n",
        "\n",
        "    # Expand abbreviations\n",
        "    if config.expand_abbreviations:\n",
        "        abbreviation_mapping = {\n",
        "            \"BRB\": \"Be Right Back\",\n",
        "            \"BTW\": \"By the Way\",\n",
        "            \"OMG\": \"Oh My God/Goodness\",\n",
        "            \"IDK\": \"I Don’t Know\",\n",
        "            \"TTYL\": \"Talk to You Later\",\n",
        "            \"OMW\": \"On My Way\",\n",
        "            \"SMH\": \"Shaking My Head\",\n",
        "            \"LOL\": \"Laugh Out Loud\",\n",
        "            \"TBD\": \"To be Determined\",\n",
        "            \"IMHO/IMO\": \"In My Humble Opinion/In My Opinion\",\n",
        "            \"HMU\": \"Hit Me Up\",\n",
        "            \"LMK\": \"Let Me Know\",\n",
        "            \"OG\": \"Original Gangsters (used for old friends)\",\n",
        "            \"FTW\": \"For The Win\",\n",
        "            \"NVM\": \"Nevermind\",\n",
        "            \"OOTD\": \"Outfit of the Day\",\n",
        "            \"FWIW\": \"For What It’s Worth\",\n",
        "            \"NGL\": \"Not Gonna Lie\",\n",
        "            \"RQ\": \"Real Quick\",\n",
        "            \"IYKYK\": \"If You Know, You Know\",\n",
        "            \"ONG\": \"On God (I Swear)\",\n",
        "            \"BRT\": \"Be Right There\",\n",
        "            \"SM\": \"So Much\",\n",
        "            \"IG\": \"I Guess\",\n",
        "            \"WYA\": \"Where You At\",\n",
        "            \"ISTG\": \"I Swear to God\",\n",
        "            \"HBU\": \"How About You\",\n",
        "            \"ATM\": \"At the Moment\",\n",
        "            \"NP\": \"No Problem\",\n",
        "            \"FOMO\": \"Fear of Missing Out\",\n",
        "            \"OBV\": \"Obviously\",\n",
        "            \"RN\": \"Right Now\"\n",
        "        }\n",
        "        tokens = [abbreviation_mapping.get(token, token) for token in tokens]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def load_and_prepare_data(config):\n",
        "    \"\"\"Load data and apply preprocessing\"\"\"\n",
        "    print(f\"Loading data from: {config.data_path}\")\n",
        "    data = pd.read_csv(config.data_path)\n",
        "\n",
        "    print(f\"Dataset shape: {data.shape}\")\n",
        "    print(f\"Target distribution:\\n{data['Target'].value_counts()}\")\n",
        "\n",
        "    # Apply preprocessing\n",
        "    print(\"Applying text preprocessing...\")\n",
        "    processed_texts = [preprocess_text(text, config) for text in tqdm(data['Title'])]\n",
        "\n",
        "    # Split data\n",
        "    train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "        processed_texts, data['Target'], test_size=0.2, random_state=42, stratify=data['Target']\n",
        "    )\n",
        "\n",
        "    return train_data, test_data, train_labels, test_labels"
      ],
      "metadata": {
        "id": "92o0qOsqueUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. MODEL & TRAINING CLASSES"
      ],
      "metadata": {
        "id": "KwSLkx55uh2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Simple Classifier\n",
        "class SimpleClassifier(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(SimpleClassifier, self).__init__()\n",
        "        self.fc = nn.Linear(input_size, output_size)\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Experiment Runner\n",
        "class ExperimentRunner:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "        # Load tokenizer based on model\n",
        "        if 'mental' in config.model_name:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
        "        else:\n",
        "            print(f\"Loading tokenizer for RoBERTa...\")\n",
        "            self.tokenizer = RobertaTokenizer.from_pretrained(config.model_name)\n",
        "\n",
        "    def prepare_dataloaders(self, train_data, test_data, train_labels, test_labels):\n",
        "        \"\"\"Tokenize data and create dataloaders\"\"\"\n",
        "        print(\"Tokenizing training data...\")\n",
        "        train_inputs = self.tokenizer(\n",
        "            train_data, padding=True, truncation=True,\n",
        "            return_tensors='pt', max_length=self.config.max_length\n",
        "        )\n",
        "        print(\"Tokenizing test data...\")\n",
        "        test_inputs = self.tokenizer(\n",
        "            test_data, padding=True, truncation=True,\n",
        "            return_tensors='pt', max_length=self.config.max_length\n",
        "        )\n",
        "        # Encode labels\n",
        "        label_encoder = LabelEncoder()\n",
        "        train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
        "        test_labels_encoded = label_encoder.transform(test_labels)\n",
        "        # Create datasets\n",
        "        train_dataset = TensorDataset(\n",
        "            train_inputs.input_ids,\n",
        "            train_inputs.attention_mask,\n",
        "            torch.tensor(train_labels_encoded)\n",
        "        )\n",
        "        test_dataset = TensorDataset(\n",
        "            test_inputs.input_ids,\n",
        "            test_inputs.attention_mask,\n",
        "            torch.tensor(test_labels_encoded)\n",
        "        )\n",
        "        # Create dataloaders\n",
        "        train_dataloader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=self.config.batch_size,\n",
        "            sampler=RandomSampler(train_dataset)\n",
        "        )\n",
        "        test_dataloader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=self.config.eval_batch_size,\n",
        "            sampler=RandomSampler(test_dataset)\n",
        "        )\n",
        "        return train_dataloader, test_dataloader, label_encoder\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Load the appropriate model\"\"\"\n",
        "        if 'mental' in self.config.model_name:\n",
        "            print(\"Loading MentalRoBERTa model...\")\n",
        "            model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                self.config.model_name,\n",
        "                num_labels=self.config.num_labels,\n",
        "                ignore_mismatched_sizes=True\n",
        "            )\n",
        "        else:\n",
        "            print(\"Loading RoBERTa model...\")\n",
        "            model = RobertaForSequenceClassification.from_pretrained(\n",
        "                'roberta-base',\n",
        "                num_labels=self.config.num_labels\n",
        "            )\n",
        "        return model.to(self.device)\n",
        "\n",
        "    def train_fine_tuning(self, model, train_dataloader):\n",
        "        \"\"\"Fine-tune the complete model\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"FINE-TUNING PHASE\")\n",
        "        print(\"=\"*50)\n",
        "        optimizer = AdamW(model.parameters(), lr=self.config.learning_rate)\n",
        "        fine_tune_history = []\n",
        "        for epoch in range(self.config.fine_tune_epochs):\n",
        "            model.train()\n",
        "            total_loss = 0\n",
        "            total_correct = 0\n",
        "            total_samples = 0\n",
        "            progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{self.config.fine_tune_epochs}')\n",
        "            for batch in progress_bar:\n",
        "                input_ids = batch[0].to(self.device)\n",
        "                attention_mask = batch[1].to(self.device)\n",
        "                labels = batch[2].to(self.device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "                # Calculate accuracy\n",
        "                logits = outputs.logits\n",
        "                predictions = torch.argmax(logits, dim=1)\n",
        "                batch_correct = (predictions == labels).sum().item()\n",
        "                total_correct += batch_correct\n",
        "                total_samples += labels.size(0)\n",
        "                progress_bar.set_postfix({\n",
        "                    'loss': f'{loss.item():.4f}',\n",
        "                    'acc': f'{(batch_correct/labels.size(0)*100):.1f}%'\n",
        "                })\n",
        "            epoch_loss = total_loss / len(train_dataloader)\n",
        "            epoch_acc = total_correct / total_samples * 100\n",
        "            fine_tune_history.append({\n",
        "                'epoch': epoch + 1,\n",
        "                'loss': epoch_loss,\n",
        "                'accuracy': epoch_acc\n",
        "            })\n",
        "            print(f'Epoch {epoch+1}: Loss = {epoch_loss:.4f}, Accuracy = {epoch_acc:.2f}%')\n",
        "        return fine_tune_history\n",
        "\n",
        "    # REMOVED the entire `train_classifier` method.\n",
        "\n",
        "    def evaluate(self, model, test_dataloader):\n",
        "        \"\"\"Evaluate the model on test set\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"EVALUATION\")\n",
        "        print(\"=\"*50)\n",
        "        model.eval()\n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "        total_loss = 0\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(test_dataloader, desc='Evaluating'):\n",
        "                input_ids = batch[0].to(self.device)\n",
        "                attention_mask = batch[1].to(self.device)\n",
        "                labels = batch[2].to(self.device)\n",
        "                # Forward pass through the complete model\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                logits = outputs.logits\n",
        "                loss = criterion(logits, labels)\n",
        "                total_loss += loss.item()\n",
        "                predictions = torch.argmax(logits, dim=1)\n",
        "                all_predictions.extend(predictions.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "        # Calculate metrics\n",
        "        test_loss = total_loss / len(test_dataloader)\n",
        "        accuracy = accuracy_score(all_labels, all_predictions)\n",
        "        precision = precision_score(all_labels, all_predictions, average='weighted')\n",
        "        recall = recall_score(all_labels, all_predictions, average='weighted')\n",
        "        f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
        "        metrics = {\n",
        "            'test_loss': test_loss,\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1\n",
        "        }\n",
        "        print(f\"Test Loss: {test_loss:.4f}\")\n",
        "        print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall: {recall:.4f}\")\n",
        "        print(f\"F1-Score: {f1:.4f}\")\n",
        "        return metrics, all_predictions, all_labels\n",
        "\n",
        "    def plot_confusion_matrix(self, predictions, labels, label_encoder, exp_name):\n",
        "        \"\"\"Plot and save confusion matrix\"\"\"\n",
        "        cm = confusion_matrix(labels, predictions)\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=label_encoder.classes_,\n",
        "                   yticklabels=label_encoder.classes_)\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('Actual')\n",
        "        plt.title(f'Confusion Matrix - {exp_name}')\n",
        "        # Save plot\n",
        "        cm_path = os.path.join(self.config.exp_dir, 'confusion_matrix.png')\n",
        "        plt.savefig(cm_path, bbox_inches='tight', dpi=300)\n",
        "        plt.show()\n",
        "        return cm\n",
        "\n",
        "    def save_results(self, fine_tune_history, metrics, label_encoder):\n",
        "        \"\"\"Save all experiment results\"\"\"\n",
        "        results = {\n",
        "            'config': {\n",
        "                'model_name': self.config.model_name,\n",
        "                'preprocessing': {\n",
        "                    'remove_stopwords': self.config.remove_stopwords,\n",
        "                    'use_stemming': self.config.use_stemming\n",
        "                },\n",
        "                'training_params': {\n",
        "                    'learning_rate': self.config.learning_rate,\n",
        "                    'fine_tune_epochs': self.config.fine_tune_epochs,\n",
        "                }\n",
        "            },\n",
        "            'fine_tune_history': fine_tune_history,\n",
        "            'metrics': metrics\n",
        "        }\n",
        "        # Save results as JSON\n",
        "        results_path = os.path.join(self.config.exp_dir, 'results.json')\n",
        "        with open(results_path, 'w') as f:\n",
        "            json.dump(results, f, indent=2)\n",
        "        # Save label encoder\n",
        "        le_path = os.path.join(self.config.exp_dir, 'label_encoder.pkl')\n",
        "        with open(le_path, 'wb') as f:\n",
        "            pickle.dump(label_encoder, f)\n",
        "        print(f\"Results saved to: {self.config.exp_dir}\")"
      ],
      "metadata": {
        "id": "XlqycmOFusOi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. EXPERIMENT 1: RoBERTa (Basic Preprocessing)"
      ],
      "metadata": {
        "id": "yLzn_8EruxoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXPERIMENT 1: RoBERTa Baseline\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Setup configuration\n",
        "config = Config().update_for_experiment(\n",
        "    exp_name=\"roberta_baseline\",\n",
        "    model_name=\"roberta-base\",\n",
        "    preprocessing_type=\"basic\"\n",
        ")\n",
        "\n",
        "# Initialize runner\n",
        "runner = ExperimentRunner(config)\n",
        "\n",
        "# Load and prepare data\n",
        "train_data, test_data, train_labels, test_labels = load_and_prepare_data(config)\n",
        "train_dataloader, test_dataloader, label_encoder = runner.prepare_dataloaders(\n",
        "    train_data, test_data, train_labels, test_labels\n",
        ")\n",
        "\n",
        "# Load model\n",
        "model = runner.load_model()\n",
        "\n",
        "# Train model\n",
        "fine_tune_history = runner.train_fine_tuning(model, train_dataloader)\n",
        "\n",
        "# Evaluate\n",
        "metrics, predictions, true_labels = runner.evaluate(model, test_dataloader)\n",
        "\n",
        "# Plot confusion matrix\n",
        "cm = runner.plot_confusion_matrix(predictions, true_labels, label_encoder, config.exp_name)\n",
        "\n",
        "# Save results\n",
        "runner.save_results(fine_tune_history, metrics, label_encoder)"
      ],
      "metadata": {
        "id": "W8rsaVQGvAci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. EXPERIMENT 2: RoBERTa Advanced Preprocessing"
      ],
      "metadata": {
        "id": "-vOszC75vNVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXPERIMENT 2: RoBERTa with Advanced Preprocessing\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Setup configuration\n",
        "config = Config().update_for_experiment(\n",
        "    exp_name=\"roberta_advanced\",\n",
        "    model_name=\"roberta-base\",\n",
        "    preprocessing_type=\"advanced\"\n",
        ")\n",
        "\n",
        "# Initialize runner\n",
        "runner = ExperimentRunner(config)\n",
        "\n",
        "# Load and prepare data\n",
        "train_data, test_data, train_labels, test_labels = load_and_prepare_data(config)\n",
        "train_dataloader, test_dataloader, label_encoder = runner.prepare_dataloaders(\n",
        "    train_data, test_data, train_labels, test_labels\n",
        ")\n",
        "\n",
        "# Load model\n",
        "model = runner.load_model()\n",
        "\n",
        "# Train and evaluate\n",
        "fine_tune_history = runner.train_fine_tuning(model, train_dataloader)\n",
        "metrics, predictions, true_labels = runner.evaluate(model, test_dataloader)\n",
        "cm = runner.plot_confusion_matrix(predictions, true_labels, label_encoder, config.exp_name)\n",
        "runner.save_results(fine_tune_history, metrics, label_encoder)"
      ],
      "metadata": {
        "id": "bULnPp6WvIUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. EXPERIMENT 3: MentalRoBERTa (Basic Preprocessing)"
      ],
      "metadata": {
        "id": "67F2uIpmvb7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXPERIMENT 3: MentalRoBERTa Baseline\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Authenticate for MentalRoBERTa (need to run this only once per session)\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n",
        "\n",
        "# Setup configuration\n",
        "config = Config().update_for_experiment(\n",
        "    exp_name=\"mentalroberta_baseline\",\n",
        "    model_name=\"mental/mental-roberta-base\",\n",
        "    preprocessing_type=\"basic\"\n",
        ")\n",
        "\n",
        "# Initialize runner\n",
        "runner = ExperimentRunner(config)\n",
        "\n",
        "# Load and prepare data\n",
        "train_data, test_data, train_labels, test_labels = load_and_prepare_data(config)\n",
        "train_dataloader, test_dataloader, label_encoder = runner.prepare_dataloaders(\n",
        "    train_data, test_data, train_labels, test_labels\n",
        ")\n",
        "\n",
        "# Load model\n",
        "model = runner.load_model()\n",
        "\n",
        "# Train and evaluate\n",
        "fine_tune_history = runner.train_fine_tuning(model, train_dataloader)\n",
        "metrics, predictions, true_labels = runner.evaluate(model, test_dataloader)\n",
        "cm = runner.plot_confusion_matrix(predictions, true_labels, label_encoder, config.exp_name)\n",
        "runner.save_results(fine_tune_history, metrics, label_encoder)"
      ],
      "metadata": {
        "id": "EhJfxvq7vVxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. EXPERIMENT 4: MentalRoBERTa Advanced Preprocessing"
      ],
      "metadata": {
        "id": "M5nGl4zpvebS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXPERIMENT 4: MentalRoBERTa with Advanced Preprocessing\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Setup configuration\n",
        "config = Config().update_for_experiment(\n",
        "    exp_name=\"mentalroberta_advanced\",\n",
        "    model_name=\"mental/mental-roberta-base\",\n",
        "    preprocessing_type=\"advanced\"\n",
        ")\n",
        "\n",
        "# Initialize runner\n",
        "runner = ExperimentRunner(config)\n",
        "\n",
        "# Load and prepare data\n",
        "train_data, test_data, train_labels, test_labels = load_and_prepare_data(config)\n",
        "train_dataloader, test_dataloader, label_encoder = runner.prepare_dataloaders(\n",
        "    train_data, test_data, train_labels, test_labels\n",
        ")\n",
        "\n",
        "# Load model\n",
        "model = runner.load_model()\n",
        "\n",
        "# Train and evaluate\n",
        "fine_tune_history = runner.train_fine_tuning(model, train_dataloader)\n",
        "metrics, predictions, true_labels = runner.evaluate(model, test_dataloader)\n",
        "cm = runner.plot_confusion_matrix(predictions, true_labels, label_encoder, config.exp_name)\n",
        "runner.save_results(fine_tune_history, metrics, label_encoder)"
      ],
      "metadata": {
        "id": "OX0tCWXHvep4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. RESULTS COMPARISON & VISUALIZATION"
      ],
      "metadata": {
        "id": "NK4R7k25vwMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "def load_and_compare_results(results_dir='/content/results'):\n",
        "    \"\"\"Load and compare results from all experiments\"\"\"\n",
        "    experiment_dirs = glob.glob(os.path.join(results_dir, '*'))\n",
        "\n",
        "    comparison_data = []\n",
        "\n",
        "    for exp_dir in experiment_dirs:\n",
        "        exp_name = os.path.basename(exp_dir)\n",
        "        results_path = os.path.join(exp_dir, 'results.json')\n",
        "\n",
        "        if os.path.exists(results_path):\n",
        "            with open(results_path, 'r') as f:\n",
        "                results = json.load(f)\n",
        "\n",
        "            comparison_data.append({\n",
        "                'Experiment': exp_name,\n",
        "                'Model': results['config']['model_name'].split('/')[-1],\n",
        "                'Preprocessing': 'Advanced' if results['config']['preprocessing']['remove_stopwords'] else 'Basic',\n",
        "                'Accuracy': results['metrics']['accuracy'],\n",
        "                'Precision': results['metrics']['precision'],\n",
        "                'Recall': results['metrics']['recall'],\n",
        "                'F1-Score': results['metrics']['f1_score'],\n",
        "                'Test Loss': results['metrics']['test_loss']\n",
        "            })\n",
        "\n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "    return comparison_df\n",
        "\n",
        "def plot_comparison(comparison_df, results_dir):\n",
        "    \"\"\"Create visual comparison of all experiments\"\"\"\n",
        "\n",
        "    # Bar plot for accuracy comparison\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "    # Accuracy comparison\n",
        "    axes[0, 0].barh(range(len(comparison_df)), comparison_df['Accuracy'])\n",
        "    axes[0, 0].set_yticks(range(len(comparison_df)))\n",
        "    axes[0, 0].set_yticklabels(comparison_df['Experiment'])\n",
        "    axes[0, 0].set_xlabel('Accuracy')\n",
        "    axes[0, 0].set_title('Accuracy Comparison')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # F1-Score comparison\n",
        "    axes[0, 1].barh(range(len(comparison_df)), comparison_df['F1-Score'])\n",
        "    axes[0, 1].set_yticks(range(len(comparison_df)))\n",
        "    axes[0, 1].set_yticklabels(comparison_df['Experiment'])\n",
        "    axes[0, 1].set_xlabel('F1-Score')\n",
        "    axes[0, 1].set_title('F1-Score Comparison')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Grouped bar chart\n",
        "    metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "    x = np.arange(len(comparison_df))\n",
        "    width = 0.2\n",
        "\n",
        "    for i, metric in enumerate(metrics_to_plot):\n",
        "        axes[1, 0].bar(x + i*width, comparison_df[metric], width, label=metric)\n",
        "\n",
        "    axes[1, 0].set_xticks(x + width*1.5)\n",
        "    axes[1, 0].set_xticklabels(comparison_df['Experiment'], rotation=45)\n",
        "    axes[1, 0].set_ylabel('Score')\n",
        "    axes[1, 0].set_title('All Metrics Comparison')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Heatmap of metrics\n",
        "    metrics_df = comparison_df.set_index('Experiment')[metrics_to_plot]\n",
        "    sns.heatmap(metrics_df, annot=True, fmt='.3f', cmap='YlOrRd', ax=axes[1, 1])\n",
        "    axes[1, 1].set_title('Metrics Heatmap')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(results_dir, 'comparison_summary.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    return comparison_df\n",
        "\n",
        "# Load and compare results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RESULTS COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Define the results directory (same as in Config class)\n",
        "RESULTS_DIR = '/content/results'\n",
        "\n",
        "# Load the comparison data\n",
        "comparison_df = load_and_compare_results(results_dir=RESULTS_DIR)\n",
        "\n",
        "print(\"\\nComparison DataFrame:\")\n",
        "print(comparison_df.to_string())\n",
        "\n",
        "# Create comparison table\n",
        "print(\"\\n FINAL RESULTS SUMMARY:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"{'Experiment':<30} {'Accuracy':<10} {'F1-Score':<10} {'Model':<20} {'Preprocessing':<15}\")\n",
        "print(\"-\" * 80)\n",
        "for idx, row in comparison_df.iterrows():\n",
        "    print(f\"{row['Experiment']:<30} {row['Accuracy']:<10.4f} {row['F1-Score']:<10.4f} {row['Model']:<20} {row['Preprocessing']:<15}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Find best model\n",
        "if len(comparison_df) > 0:\n",
        "    best_idx = comparison_df['F1-Score'].idxmax()\n",
        "    best_model = comparison_df.loc[best_idx]\n",
        "    print(f\"\\n BEST MODEL: {best_model['Experiment']}\")\n",
        "    print(f\"   Accuracy: {best_model['Accuracy']:.4f}\")\n",
        "    print(f\"   F1-Score: {best_model['F1-Score']:.4f}\")\n",
        "    print(f\"   Model: {best_model['Model']}\")\n",
        "    print(f\"   Preprocessing: {best_model['Preprocessing']}\")\n",
        "\n",
        "    # Visual comparison\n",
        "    plot_comparison(comparison_df, RESULTS_DIR)\n",
        "else:\n",
        "    print(\"\\n No results found. Please run the experiments first.\")"
      ],
      "metadata": {
        "id": "qMXC1LNaRLZf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}