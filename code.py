# -*- coding: utf-8 -*-
"""Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nEH0otF8agX2FkDDygEo4xXlmDchTbQh
"""

#Initial RoBERTa
# Install the Transformers library
!pip install transformers

# Import necessary libraries
import re
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import wordnet
from google.colab import drive
import os
import torch
from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW
from torch.utils.data import DataLoader, TensorDataset, RandomSampler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import random
import seaborn as sns
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt


# Mount Google Drive
drive.mount('/content/drive')

# Navigate to the directory containing dataset
os.chdir('/content/drive/My Drive/')

# Download NLTK resources
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# Load the CSV file into a Pandas DataFrame
data = pd.read_csv('Final Cleaned Dataset.csv')

# Reset the indices to avoid potential issues
data = data.reset_index(drop=True)

# Function to preprocess text
def preprocess_text(text):
    # Remove URLs
    text = re.sub(r'https?://\S+|www\.\S+', '', text)

    # Lemmatization
    tokens = word_tokenize(text)
    lemmatizer = nltk.WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]

    # Expand contractions
    contraction_mapping = {
        "n't": " not",
        "'s": " is",
        "'m": " am",
        "'re": " are",
        "'ve": " have",
        "'ll": " will"
    }
    tokens = [contraction_mapping.get(token, token) for token in tokens]

    # Expand abbreviations
    abbreviation_mapping = {
        "BRB": "Be Right Back",
        "BTW": "By the Way",
        "OMG": "Oh My God/Goodness",
        "IDK": "I Don’t Know",
        "TTYL": "Talk to You Later",
        "OMW": "On My Way",
        "SMH": "Shaking My Head",
        "LOL": "Laugh Out Loud",
        "TBD": "To be Determined",
        "IMHO/IMO": "In My Humble Opinion/In My Opinion",
        "HMU": "Hit Me Up",
        "LMK": "Let Me Know",
        "OG": "Original Gangsters (used for old friends)",
        "FTW": "For The Win",
        "NVM": "Nevermind",
        "OOTD": "Outfit of the Day",
        "FWIW": "For What It’s Worth",
        "NGL": "Not Gonna Lie",
        "RQ": "Real Quick",
        "IYKYK": "If You Know, You Know",
        "ONG": "On God (I Swear)",
        "BRT": "Be Right There",
        "SM": "So Much",
        "IG": "I Guess",
        "WYA": "Where You At",
        "ISTG": "I Swear to God",
        "HBU": "How About You",
        "ATM": "At the Moment",
        "NP": "No Problem",
        "FOMO": "Fear of Missing Out",
        "OBV": "Obviously",
        "RN": "Right Now"
    }
    tokens = [abbreviation_mapping.get(token, token) for token in tokens]

    return ' '.join(tokens)

# Function to get the part of speech for WordNet lemmatization
def get_wordnet_pos(token):
    tag = nltk.pos_tag([token])[0][1][0].upper()
    tag_dict = {"N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV, "J": wordnet.ADJ}
    return tag_dict.get(tag, wordnet.NOUN)

# Process the text dataset
text_dataset = data['Title']
processed_dataset = [preprocess_text(text) if isinstance(text, str) else '' for text in text_dataset]

# Split the preprocessed dataset into training and test sets
train_data, test_data, train_labels, test_labels = train_test_split(processed_dataset, data['Target'], test_size=0.2, random_state=42)

# Load the RoBERTa tokenizer and model
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=6)

# Tokenize and preprocess text data for training
train_inputs = tokenizer(train_data, padding=True, truncation=True, return_tensors='pt', max_length=128)

# Labels in string format to be encoded to integers for training
label_encoder = LabelEncoder()
train_labels = label_encoder.fit_transform(train_labels)

# Create a TensorDataset for training data
train_dataset = TensorDataset(train_inputs.input_ids, train_inputs.attention_mask, torch.tensor(train_labels))

# Combine the input data and labels in the DataLoader for training
train_dataloader = DataLoader(
    train_dataset,
    batch_size=8,
    sampler=RandomSampler(train_dataset),
)

# Tokenize and preprocess text data for testing
test_inputs = tokenizer(test_data, padding=True, truncation=True, return_tensors='pt', max_length=128)

# Labels in string format to be encoded to integers for testing
test_labels = label_encoder.transform(test_labels)

# Create a TensorDataset for test data
test_dataset = TensorDataset(test_inputs.input_ids, test_inputs.attention_mask, torch.tensor(test_labels))

# Combine the input data and labels in the DataLoader for testing
test_dataloader = DataLoader(
    test_dataset,
    batch_size=8,
    sampler=RandomSampler(test_dataset),
)

# Define hyperparameters
learning_rate = 1e-5
num_epochs_fine_tuning = 10
num_epochs_classifier = 5

# Create an optimizer and specify the model's device
optimizer = AdamW(model.parameters(), lr=learning_rate)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Fine-tune the model for the specified number of epochs
for epoch in range(num_epochs_fine_tuning):
    model.train()
    total_loss = 0
    total_correct = 0
    total_samples = 0

    for batch in train_dataloader:
        input_ids = batch[0].to(device)
        attention_mask = batch[1].to(device)
        labels = batch[2].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

        # Calculate training accuracy
        logits = outputs.logits
        predictions = torch.argmax(logits, dim=1)
        total_correct += (predictions == labels).sum().item()
        total_samples += labels.size(0)

    # Calculate and print training accuracy and loss as percentages
    training_accuracy = (total_correct / total_samples) * 100
    training_loss_percentage = (total_loss / len(train_dataloader)) * 100
    print(f'Epoch {epoch + 1}, Fine-tuning Loss: {training_loss_percentage:.2f}%, Fine-tuning Accuracy: {training_accuracy:.2f}%')

# Adding a simple neural network for classification
class SimpleClassifier(torch.nn.Module):
    def __init__(self, input_size, output_size):
        super(SimpleClassifier, self).__init__()
        self.fc = torch.nn.Linear(input_size, output_size)

    def forward(self, x):
        x = self.fc(x)
        return x

# Define and initialize the classifier
embedding_size = model.config.hidden_size  # Extracts embedding size
output_dimensions = model.config.num_labels
classifier = SimpleClassifier(embedding_size, output_dimensions)
classifier.to(device)

# Define the loss function and optimizer for the classifier
criterion = torch.nn.CrossEntropyLoss()
classifier_optimizer = torch.optim.Adam(classifier.parameters(), lr=learning_rate)

# Train the classifier
for epoch in range(num_epochs_classifier):
    classifier.train()
    total_loss = 0
    total_correct = 0
    total_samples = 0

    for batch in train_dataloader:
        input_ids = batch[0].to(device)
        attention_mask = batch[1].to(device)
        labels = batch[2].to(device)

        # Extract embeddings using the pre-trained RoBERTa model
        with torch.no_grad():
            embeddings = model.roberta(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]

        # Forward pass through the classifier
        classifier_outputs = classifier(embeddings)

        # Compute the loss
        loss = criterion(classifier_outputs, labels)

        # Backward pass and optimization
        classifier_optimizer.zero_grad()
        loss.backward()
        classifier_optimizer.step()

        total_loss += loss.item()

        # Calculate training accuracy
        predictions = torch.argmax(classifier_outputs, dim=1)
        total_correct += (predictions == labels).sum().item()
        total_samples += labels.size(0)

    # Calculate and print training accuracy and loss as percentages
    training_accuracy = (total_correct / total_samples) * 100
    training_loss_percentage = (total_loss / len(train_dataloader)) * 100
    print(f'Epoch {epoch + 1}, Classifier Training Loss: {training_loss_percentage:.2f}%, Classifier Training Accuracy: {training_accuracy:.2f}%')

# Evaluate the classifier on the test set
classifier.eval()
test_loss_classifier = 0
test_correct_classifier = 0
test_samples_classifier = 0
all_predictions = []
all_labels = []

for batch in test_dataloader:
    input_ids = batch[0].to(device)
    attention_mask = batch[1].to(device)
    labels = batch[2].to(device)

    # Extract embeddings using the pre-trained RoBERTa model
    with torch.no_grad():
        embeddings = model.roberta(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]

    # Forward pass through the classifier
    classifier_outputs = classifier(embeddings)

    # Compute the loss
    loss = criterion(classifier_outputs, labels)
    test_loss_classifier += loss.item()

    # Calculate test accuracy
    predictions = torch.argmax(classifier_outputs, dim=1)
    test_correct_classifier += (predictions == labels).sum().item()
    test_samples_classifier += labels.size(0)

    # Collect predictions and labels for confusion matrix
    all_predictions.extend(predictions.cpu().numpy())
    all_labels.extend(labels.cpu().numpy())

# Calculate and print test accuracy and loss as percentages for the classifier
test_accuracy_classifier = (test_correct_classifier / test_samples_classifier) * 100
test_loss_percentage_classifier = (test_loss_classifier / len(test_dataloader)) * 100
print(f'Final Classifier Test Loss: {test_loss_percentage_classifier:.2f}%, Final Classifier Test Accuracy: {test_accuracy_classifier:.2f}%')

# Calculate and print accuracy, precision, recall, and F1 score for the classifier
# Convert predictions and labels to numpy arrays
accuracy = accuracy_score(all_labels, all_predictions)
precision = precision_score(all_labels, all_predictions, average='weighted')
recall = recall_score(all_labels, all_predictions, average='weighted')
f1 = f1_score(all_labels, all_predictions, average='weighted')

print(f'Classifier Accuracy: {accuracy:.4f}')
print(f'Classifier Precision: {precision:.4f}')
print(f'Classifier Recall: {recall:.4f}')
print(f'Classifier F1 Score: {f1:.4f}')

# Plot Confusion Matrix
cm = confusion_matrix(all_labels, all_predictions)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

#FURTHER PREPROCESSED ROBERTA

# Import necessary libraries
import re
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import wordnet, stopwords
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer
from google.colab import drive
import os
import torch
from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW
from torch.utils.data import DataLoader, TensorDataset, RandomSampler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import random
import seaborn as sns
import matplotlib.pyplot as plt

# Mount Google Drive
drive.mount('/content/drive')

# Navigate to the directory containing dataset
os.chdir('/content/drive/My Drive/')

# Download NLTK resources
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')

# Load the CSV file into a Pandas DataFrame
data = pd.read_csv('Final Cleaned Dataset.csv')

# Reset the indices to avoid potential issues
data = data.reset_index(drop=True)

# Function to preprocess text
def preprocess_text(text):
    # Remove URLs
    text = re.sub(r'https?://\S+|www\.\S+', '', text)

    # Lemmatization
    tokens = word_tokenize(text)
    lemmatizer = nltk.WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]

    # Remove stopwords and perform stemming
    tokens = preprocess_tokens(tokens)

    # Expand contractions
    contraction_mapping = {
        "n't": " not",
        "'s": " is",
        "'m": " am",
        "'re": " are",
        "'ve": " have",
        "'ll": " will"
    }
    tokens = [contraction_mapping.get(token, token) for token in tokens]

    # Expand abbreviations
    abbreviation_mapping = {
        "BRB": "Be Right Back",
        "BTW": "By the Way",
        "OMG": "Oh My God/Goodness",
        "IDK": "I Don’t Know",
        "TTYL": "Talk to You Later",
        "OMW": "On My Way",
        "SMH": "Shaking My Head",
        "LOL": "Laugh Out Loud",
        "TBD": "To be Determined",
        "IMHO/IMO": "In My Humble Opinion/In My Opinion",
        "HMU": "Hit Me Up",
        "LMK": "Let Me Know",
        "OG": "Original Gangsters (used for old friends)",
        "FTW": "For The Win",
        "NVM": "Nevermind",
        "OOTD": "Outfit of the Day",
        "FWIW": "For What It’s Worth",
        "NGL": "Not Gonna Lie",
        "RQ": "Real Quick",
        "IYKYK": "If You Know, You Know",
        "ONG": "On God (I Swear)",
        "BRT": "Be Right There",
        "SM": "So Much",
        "IG": "I Guess",
        "WYA": "Where You At",
        "ISTG": "I Swear to God",
        "HBU": "How About You",
        "ATM": "At the Moment",
        "NP": "No Problem",
        "FOMO": "Fear of Missing Out",
        "OBV": "Obviously",
        "RN": "Right Now"
    }
    tokens = [abbreviation_mapping.get(token, token) for token in tokens]

    return ' '.join(tokens)

# Function to get the part of speech for WordNet lemmatization
def get_wordnet_pos(token):
    tag = nltk.pos_tag([token])[0][1][0].upper()
    tag_dict = {"N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV, "J": wordnet.ADJ}
    return tag_dict.get(tag, wordnet.NOUN)

 # Function to remove stopwords
def preprocess_tokens(tokens):
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word.lower() not in stop_words]

    # Stemming
    stemmer = PorterStemmer()
    tokens = [stemmer.stem(word) for word in tokens]

    return tokens

# Process the text dataset using the preprocessing function
text_dataset = data['Title']
processed_dataset = [preprocess_text(text) if isinstance(text, str) else '' for text in text_dataset]

# Split the preprocessed dataset into training and test sets
train_data, test_data, train_labels, test_labels = train_test_split(processed_dataset, data['Target'], test_size=0.2, random_state=42)

# Load the RoBERTa tokenizer and model
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=6)

# Tokenize and preprocess text data for training
train_inputs = tokenizer(train_data, padding=True, truncation=True, return_tensors='pt', max_length=128)

# Labels in string format to be encoded to integers for training
label_encoder = LabelEncoder()
train_labels = label_encoder.fit_transform(train_labels)

# Create a TensorDataset for training data
train_dataset = TensorDataset(train_inputs.input_ids, train_inputs.attention_mask, torch.tensor(train_labels))

# Combine the input data and labels in the DataLoader for training
train_dataloader = DataLoader(
    train_dataset,
    batch_size=8,
    sampler=RandomSampler(train_dataset),
)

# Tokenize and preprocess text data for testing
test_inputs = tokenizer(test_data, padding=True, truncation=True, return_tensors='pt', max_length=128)

# Labels in string format to be encoded to integers for testing
test_labels = label_encoder.transform(test_labels)

# Create a TensorDataset for test data
test_dataset = TensorDataset(test_inputs.input_ids, test_inputs.attention_mask, torch.tensor(test_labels))

# Combine the input data and labels in the DataLoader for testing
test_dataloader = DataLoader(
    test_dataset,
    batch_size=32,
    sampler=RandomSampler(test_dataset),
)

# Define hyperparameters
learning_rate = 1e-5
num_epochs_fine_tuning = 10
num_epochs_classifier = 5

# Create an optimizer and specify the model's device
optimizer = AdamW(model.parameters(), lr=learning_rate)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Fine-tune the model for the specified number of epochs
for epoch in range(num_epochs_fine_tuning):
    model.train()
    total_loss = 0
    total_correct = 0
    total_samples = 0

    for batch in train_dataloader:
        input_ids = batch[0].to(device)
        attention_mask = batch[1].to(device)
        labels = batch[2].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

        # Calculate training accuracy
        logits = outputs.logits
        predictions = torch.argmax(logits, dim=1)
        total_correct += (predictions == labels).sum().item()
        total_samples += labels.size(0)

    # Calculate and print training accuracy and loss as percentages
    training_accuracy = (total_correct / total_samples) * 100
    training_loss_percentage = (total_loss / len(train_dataloader)) * 100
    print(f'Epoch {epoch + 1}, Fine-tuning Loss: {training_loss_percentage:.2f}%, Fine-tuning Accuracy: {training_accuracy:.2f}%')

# Adding a simple neural network for classification
class SimpleClassifier(torch.nn.Module):
    def __init__(self, input_size, output_size):
        super(SimpleClassifier, self).__init__()
        self.fc = torch.nn.Linear(input_size, output_size)

    def forward(self, x):
        x = self.fc(x)
        return x

# Define and initialize the classifier
embedding_size = model.config.hidden_size
output_dimensions = model.config.num_labels
classifier = SimpleClassifier(embedding_size, output_dimensions)
classifier.to(device)

# Define the loss function and optimizer for the classifier
criterion = torch.nn.CrossEntropyLoss()
classifier_optimizer = torch.optim.Adam(classifier.parameters(), lr=learning_rate)

# Train the classifier
for epoch in range(num_epochs_classifier):
    classifier.train()
    total_loss = 0
    total_correct = 0
    total_samples = 0

    for batch in train_dataloader:
        input_ids = batch[0].to(device)
        attention_mask = batch[1].to(device)
        labels = batch[2].to(device)

        # Extract embeddings using the pre-trained RoBERTa model
        with torch.no_grad():
            embeddings = model.roberta(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]

        # Forward pass through the classifier
        classifier_outputs = classifier(embeddings)

        # Compute the loss
        loss = criterion(classifier_outputs, labels)

        # Backward pass and optimization
        classifier_optimizer.zero_grad()
        loss.backward()
        classifier_optimizer.step()

        total_loss += loss.item()

        # Calculate training accuracy
        predictions = torch.argmax(classifier_outputs, dim=1)
        total_correct += (predictions == labels).sum().item()
        total_samples += labels.size(0)

    # Calculate and print training accuracy and loss as percentages
    training_accuracy = (total_correct / total_samples) * 100
    training_loss_percentage = (total_loss / len(train_dataloader)) * 100
    print(f'Epoch {epoch + 1}, Classifier Training Loss: {training_loss_percentage:.2f}%, Classifier Training Accuracy: {training_accuracy:.2f}%')

# Evaluate the classifier on the test set
classifier.eval()
test_loss_classifier = 0
test_correct_classifier = 0
test_samples_classifier = 0
all_predictions = []
all_labels = []

for batch in test_dataloader:
    input_ids = batch[0].to(device)
    attention_mask = batch[1].to(device)
    labels = batch[2].to(device)

    # Extract embeddings using the pre-trained RoBERTa model
    with torch.no_grad():
        embeddings = model.roberta(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]

    # Forward pass through the classifier
    classifier_outputs = classifier(embeddings)

    # Compute the loss
    loss = criterion(classifier_outputs, labels)
    test_loss_classifier += loss.item()

    # Calculate test accuracy
    predictions = torch.argmax(classifier_outputs, dim=1)
    test_correct_classifier += (predictions == labels).sum().item()
    test_samples_classifier += labels.size(0)

    # Collect predictions and labels for confusion matrix
    all_predictions.extend(predictions.cpu().numpy())
    all_labels.extend(labels.cpu().numpy())

# Calculate and print test accuracy and loss as percentages for the classifier
test_accuracy_classifier = (test_correct_classifier / test_samples_classifier) * 100
test_loss_percentage_classifier = (test_loss_classifier / len(test_dataloader)) * 100
print(f'Final Classifier Test Loss: {test_loss_percentage_classifier:.2f}%, Final Classifier Test Accuracy: {test_accuracy_classifier:.2f}%')

# Calculate and print accuracy, precision, recall, and F1 score for the classifier
# Convert predictions and labels to numpy arrays
accuracy = accuracy_score(all_labels, all_predictions)
precision = precision_score(all_labels, all_predictions, average='weighted')
recall = recall_score(all_labels, all_predictions, average='weighted')
f1 = f1_score(all_labels, all_predictions, average='weighted')

print(f'Classifier Accuracy: {accuracy:.4f}')
print(f'Classifier Precision: {precision:.4f}')
print(f'Classifier Recall: {recall:.4f}')
print(f'Classifier F1 Score: {f1:.4f}')

# Plot Confusion Matrix
cm = confusion_matrix(all_labels, all_predictions)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

#Un preprocessed MENTALROBERTA
# Import necessary libraries
import re
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer
from nltk.corpus import wordnet, stopwords
from google.colab import drive
import os
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW
from torch.utils.data import DataLoader, TensorDataset, RandomSampler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Authenticate and install the Transformers library
!pip install transformers
!huggingface-cli login

# Import necessary libraries with authentication
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW
from torch.utils.data import DataLoader, TensorDataset, RandomSampler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Mount Google Drive
drive.mount('/content/drive')

# Navigate to the directory containing dataset
os.chdir('/content/drive/My Drive/')

# Download NLTK resources
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# Load the CSV file into a Pandas DataFrame
data = pd.read_csv('Final Cleaned Dataset.csv')

# Reset the indices to avoid potential issues
data = data.reset_index(drop=True)

# Function to preprocess text
def preprocess_text(text):
    # Remove URLs
    text = re.sub(r'https?://\S+|www\.\S+', '', text)

    # Lemmatization
    tokens = word_tokenize(text)
    lemmatizer = nltk.WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]

    # Expand contractions
    contraction_mapping = {
        "n't": " not",
        "'s": " is",
        "'m": " am",
        "'re": " are",
        "'ve": " have",
        "'ll": " will"
    }
    tokens = [contraction_mapping.get(token, token) for token in tokens]

    # Expand abbreviations
    abbreviation_mapping = {
        "BRB": "Be Right Back",
        "BTW": "By the Way",
        "OMG": "Oh My God/Goodness",
        "IDK": "I Don’t Know",
        "TTYL": "Talk to You Later",
        "OMW": "On My Way",
        "SMH": "Shaking My Head",
        "LOL": "Laugh Out Loud",
        "TBD": "To be Determined",
        "IMHO/IMO": "In My Humble Opinion/In My Opinion",
        "HMU": "Hit Me Up",
        "LMK": "Let Me Know",
        "OG": "Original Gangsters (used for old friends)",
        "FTW": "For The Win",
        "NVM": "Nevermind",
        "OOTD": "Outfit of the Day",
        "FWIW": "For What It’s Worth",
        "NGL": "Not Gonna Lie",
        "RQ": "Real Quick",
        "IYKYK": "If You Know, You Know",
        "ONG": "On God (I Swear)",
        "BRT": "Be Right There",
        "SM": "So Much",
        "IG": "I Guess",
        "WYA": "Where You At",
        "ISTG": "I Swear to God",
        "HBU": "How About You",
        "ATM": "At the Moment",
        "NP": "No Problem",
        "FOMO": "Fear of Missing Out",
        "OBV": "Obviously",
        "RN": "Right Now"
    }
    tokens = [abbreviation_mapping.get(token, token) for token in tokens]

    return ' '.join(tokens)

# Function to get the part of speech for WordNet lemmatization
def get_wordnet_pos(token):
    tag = nltk.pos_tag([token])[0][1][0].upper()
    tag_dict = {"N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV, "J": wordnet.ADJ}
    return tag_dict.get(tag, wordnet.NOUN)

# Process the text dataset
text_dataset = data['Title']
processed_dataset = [preprocess_text(text) if isinstance(text, str) else '' for text in text_dataset]

# Split the preprocessed dataset into training and test sets
train_data, test_data, train_labels, test_labels = train_test_split(processed_dataset, data['Target'], test_size=0.2, random_state=42)

# Load the MentalRoBERTa tokenizer and model with authentication
tokenizer = AutoTokenizer.from_pretrained("mental/mental-roberta-base")
model = AutoModelForSequenceClassification.from_pretrained("mental/mental-roberta-base", num_labels=len(set(train_labels)))

# Tokenize and preprocess text data for training
train_inputs = tokenizer(train_data, padding=True, truncation=True, return_tensors='pt', max_length=128)

# Labels in string format to be encoded to integers for training
label_encoder = LabelEncoder()
train_labels = label_encoder.fit_transform(train_labels)

# Create a TensorDataset for training data
train_dataset = TensorDataset(train_inputs.input_ids, train_inputs.attention_mask, torch.tensor(train_labels))

# Combine the input data and labels in the DataLoader for training
train_dataloader = DataLoader(
    train_dataset,
    batch_size=8,
    sampler=RandomSampler(train_dataset),
)

# Tokenize and preprocess text data for testing
test_inputs = tokenizer(test_data, padding=True, truncation=True, return_tensors='pt', max_length=128)

# Labels in string format to be encoded to integers for testing
test_labels = label_encoder.transform(test_labels)

# Create a TensorDataset for testing data
test_dataset = TensorDataset(test_inputs.input_ids, test_inputs.attention_mask, torch.tensor(test_labels))

# Combine the input data and labels in the DataLoader for testing
test_dataloader = DataLoader(
    test_dataset,
    batch_size=32,
    sampler=RandomSampler(test_dataset),
)

# Fine-tune the model for the specified number of epochs
num_epochs_fine_tuning = 10
num_epochs_classifier = 5

# Create an optimizer and specify the model's device
optimizer = AdamW(model.parameters(), lr=1e-5)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Fine-tune the model for the specified number of epochs
for epoch in range(num_epochs_fine_tuning):
    model.train()
    total_loss = 0
    total_correct = 0
    total_samples = 0

    for batch in train_dataloader:
        input_ids = batch[0].to(device)
        attention_mask = batch[1].to(device)
        labels = batch[2].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

        # Calculate training accuracy
        logits = outputs.logits
        predictions = torch.argmax(logits, dim=1)
        total_correct += (predictions == labels).sum().item()
        total_samples += labels.size(0)

    # Calculate and print training accuracy and loss as percentages
    training_accuracy = (total_correct / total_samples) * 100
    training_loss_percentage = (total_loss / len(train_dataloader)) * 100
    print(f'Epoch {epoch + 1}, Fine-tuning Loss: {training_loss_percentage:.2f}%, Fine-tuning Accuracy: {training_accuracy:.2f}%')

# Adding a simple neural network for classification
class SimpleClassifier(torch.nn.Module):
    def __init__(self, input_size, output_size):
        super(SimpleClassifier, self).__init__()
        self.fc = torch.nn.Linear(input_size, output_size)

    def forward(self, x):
        x = self.fc(x)
        return x

# Define and initialize the classifier
embedding_size = model.config.hidden_size
output_dimensions = model.config.num_labels
classifier = SimpleClassifier(embedding_size, output_dimensions)
classifier.to(device)

# Define the loss function and optimizer for the classifier
criterion = torch.nn.CrossEntropyLoss()
classifier_optimizer = torch.optim.Adam(classifier.parameters(), lr=learning_rate)

# Train the classifier
for epoch in range(num_epochs_classifier):
    classifier.train()
    total_loss = 0
    total_correct = 0
    total_samples = 0

    for batch in train_dataloader:
        input_ids = batch[0].to(device)
        attention_mask = batch[1].to(device)
        labels = batch[2].to(device)

        # Extract embeddings using the pre-trained MentalRoBERTa model
        with torch.no_grad():
            embeddings = model.roberta(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]

        # Forward pass through the classifier
        classifier_outputs = classifier(embeddings)

        # Compute the loss
        loss = criterion(classifier_outputs, labels)

        # Backward pass and optimization
        classifier_optimizer.zero_grad()
        loss.backward()
        classifier_optimizer.step()

        total_loss += loss.item()

        # Calculate training accuracy
        predictions = torch.argmax(classifier_outputs, dim=1)
        total_correct += (predictions == labels).sum().item()
        total_samples += labels.size(0)

    # Calculate and print training accuracy and loss as percentages
    training_accuracy = (total_correct / total_samples) * 100
    training_loss_percentage = (total_loss / len(train_dataloader)) * 100
    print(f'Epoch {epoch + 1}, Classifier Training Loss: {training_loss_percentage:.2f}%, Classifier Training Accuracy: {training_accuracy:.2f}%')

# Evaluate the classifier on the test set
classifier.eval()
test_loss_classifier = 0
test_correct_classifier = 0
test_samples_classifier = 0
all_predictions = []
all_labels = []

for batch in test_dataloader:
    input_ids = batch[0].to(device)
    attention_mask = batch[1].to(device)
    labels = batch[2].to(device)

    # Extract embeddings using the pre-trained MentalRoBERTa model
    with torch.no_grad():
        embeddings = model.roberta(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]

    # Forward pass through the classifier
    classifier_outputs = classifier(embeddings)

    # Compute the loss
    loss = criterion(classifier_outputs, labels)
    test_loss_classifier += loss.item()

    # Calculate test accuracy
    predictions = torch.argmax(classifier_outputs, dim=1)
    test_correct_classifier += (predictions == labels).sum().item()
    test_samples_classifier += labels.size(0)

    # Collect predictions and labels for confusion matrix
    all_predictions.extend(predictions.cpu().numpy())
    all_labels.extend(labels.cpu().numpy())

# Calculate and print test accuracy and loss as percentages for the classifier
test_accuracy_classifier = (test_correct_classifier / test_samples_classifier) * 100
test_loss_percentage_classifier = (test_loss_classifier / len(test_dataloader)) * 100
print(f'Final Classifier Test Loss: {test_loss_percentage_classifier:.2f}%, Final Classifier Test Accuracy: {test_accuracy_classifier:.2f}%')

# Calculate and print accuracy, precision, recall, and F1 score for the classifier
# Convert predictions and labels to numpy arrays
accuracy = accuracy_score(all_labels, all_predictions)
precision = precision_score(all_labels, all_predictions, average='weighted')
recall = recall_score(all_labels, all_predictions, average='weighted')
f1 = f1_score(all_labels, all_predictions, average='weighted')

print(f'Classifier Accuracy: {accuracy:.4f}')
print(f'Classifier Precision: {precision:.4f}')
print(f'Classifier Recall: {recall:.4f}')
print(f'Classifier F1 Score: {f1:.4f}')

# Plot Confusion Matrix
cm = confusion_matrix(all_labels, all_predictions)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Further Pre-processed MentalRoBERTa

# Import necessary libraries
import re
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer
from nltk.corpus import wordnet, stopwords
from google.colab import drive
import os
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW
from torch.utils.data import DataLoader, TensorDataset, RandomSampler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Authenticate and install the Transformers library
!pip install transformers
!huggingface-cli login

# Specify the learning rate
learning_rate = 1e-5

# Import necessary libraries with authentication
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW
from torch.utils.data import DataLoader, TensorDataset, RandomSampler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Mount Google Drive
drive.mount('/content/drive')

# Navigate to the directory containing dataset
os.chdir('/content/drive/My Drive/')

# Download NLTK resources
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')

# Load the CSV file into a Pandas DataFrame
data = pd.read_csv('Final Cleaned Dataset.csv')

# Reset the indices to avoid potential issues
data = data.reset_index(drop=True)

# Function to preprocess text
def preprocess_text(text):
    # Remove URLs
    text = re.sub(r'https?://\S+|www\.\S+', '', text)

    # Lemmatization
    tokens = word_tokenize(text)
    lemmatizer = nltk.WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]

    # Remove stopwords and perform stemming
    tokens = preprocess_tokens(tokens)

    # Expand contractions
    contraction_mapping = {
        "n't": " not",
        "'s": " is",
        "'m": " am",
        "'re": " are",
        "'ve": " have",
        "'ll": " will"
    }
    tokens = [contraction_mapping.get(token, token) for token in tokens]

    # Expand abbreviations
    abbreviation_mapping = {
        "BRB": "Be Right Back",
        "BTW": "By the Way",
        "OMG": "Oh My God/Goodness",
        "IDK": "I Don’t Know",
        "TTYL": "Talk to You Later",
        "OMW": "On My Way",
        "SMH": "Shaking My Head",
        "LOL": "Laugh Out Loud",
        "TBD": "To be Determined",
        "IMHO/IMO": "In My Humble Opinion/In My Opinion",
        "HMU": "Hit Me Up",
        "LMK": "Let Me Know",
        "OG": "Original Gangsters (used for old friends)",
        "FTW": "For The Win",
        "NVM": "Nevermind",
        "OOTD": "Outfit of the Day",
        "FWIW": "For What It’s Worth",
        "NGL": "Not Gonna Lie",
        "RQ": "Real Quick",
        "IYKYK": "If You Know, You Know",
        "ONG": "On God (I Swear)",
        "BRT": "Be Right There",
        "SM": "So Much",
        "IG": "I Guess",
        "WYA": "Where You At",
        "ISTG": "I Swear to God",
        "HBU": "How About You",
        "ATM": "At the Moment",
        "NP": "No Problem",
        "FOMO": "Fear of Missing Out",
        "OBV": "Obviously",
        "RN": "Right Now"
    }
    tokens = [abbreviation_mapping.get(token, token) for token in tokens]

    return ' '.join(tokens)

# Function to get the part of speech for WordNet lemmatization
def get_wordnet_pos(token):
    tag = nltk.pos_tag([token])[0][1][0].upper()
    tag_dict = {"N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV, "J": wordnet.ADJ}
    return tag_dict.get(tag, wordnet.NOUN)

# Function to remove stopwords and perform stemming
def preprocess_tokens(tokens):
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word.lower() not in stop_words]

    # Stemming
    stemmer = PorterStemmer()
    tokens = [stemmer.stem(word) for word in tokens]

    return tokens

# Process the text dataset using the preprocessing function
text_dataset = data['Title']
processed_dataset = [preprocess_text(text) if isinstance(text, str) else '' for text in text_dataset]

# Split the preprocessed dataset into training and test sets
train_data, test_data, train_labels, test_labels = train_test_split(processed_dataset, data['Target'], test_size=0.2, random_state=42)

# Load the MentalRoBERTa tokenizer and model with authentication
tokenizer = AutoTokenizer.from_pretrained("mental/mental-roberta-base")
model = AutoModelForSequenceClassification.from_pretrained("mental/mental-roberta-base", num_labels=len(set(train_labels)))

# Tokenize and preprocess text data for training
train_inputs = tokenizer(train_data, padding=True, truncation=True, return_tensors='pt', max_length=128)

# Labels in string format to be encoded to integers for training
label_encoder = LabelEncoder()
train_labels = label_encoder.fit_transform(train_labels)

# Create a TensorDataset for training data
train_dataset = TensorDataset(train_inputs.input_ids, train_inputs.attention_mask, torch.tensor(train_labels))

# Combine the input data and labels in the DataLoader for training
train_dataloader = DataLoader(
    train_dataset,
    batch_size=8,
    sampler=RandomSampler(train_dataset),
)

# Tokenize and preprocess text data for testing
test_inputs = tokenizer(test_data, padding=True, truncation=True, return_tensors='pt', max_length=128)

# Labels in string format to be encoded to integers for testing
test_labels = label_encoder.transform(test_labels)

# Create a TensorDataset for testing data
test_dataset = TensorDataset(test_inputs.input_ids, test_inputs.attention_mask, torch.tensor(test_labels))

# Combine the input data and labels in the DataLoader for testing
test_dataloader = DataLoader(
    test_dataset,
    batch_size=32,
    sampler=RandomSampler(test_dataset),
)

# Fine-tune the model for the specified number of epochs
num_epochs_fine_tuning = 10
num_epochs_classifier = 5

# Create an optimizer and specify the model's device
optimizer = AdamW(model.parameters(), lr=learning_rate)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Fine-tune the model for the specified number of epochs
for epoch in range(num_epochs_fine_tuning):
    model.train()
    total_loss = 0
    total_correct = 0
    total_samples = 0

    for batch in train_dataloader:
        input_ids = batch[0].to(device)
        attention_mask = batch[1].to(device)
        labels = batch[2].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

        # Calculate training accuracy
        logits = outputs.logits
        predictions = torch.argmax(logits, dim=1)
        total_correct += (predictions == labels).sum().item()
        total_samples += labels.size(0)

    # Calculate and print training accuracy and loss as percentages
    training_accuracy = (total_correct / total_samples) * 100
    training_loss_percentage = (total_loss / len(train_dataloader)) * 100
    print(f'Epoch {epoch + 1}, Fine-tuning Loss: {training_loss_percentage:.2f}%, Fine-tuning Accuracy: {training_accuracy:.2f}%')

# Adding a simple neural network for classification
class SimpleClassifier(torch.nn.Module):
    def __init__(self, input_size, output_size):
        super(SimpleClassifier, self).__init__()
        self.fc = torch.nn.Linear(input_size, output_size)

    def forward(self, x):
        x = self.fc(x)
        return x

# Define and initialize the classifier
embedding_size = model.config.hidden_size
output_dimensions = model.config.num_labels
classifier = SimpleClassifier(embedding_size, output_dimensions)
classifier.to(device)

# Define the loss function and optimizer for the classifier
criterion = torch.nn.CrossEntropyLoss()
classifier_optimizer = torch.optim.Adam(classifier.parameters(), lr=learning_rate)

# Train the classifier
for epoch in range(num_epochs_classifier):
    classifier.train()
    total_loss = 0
    total_correct = 0
    total_samples = 0

    for batch in train_dataloader:
        input_ids = batch[0].to(device)
        attention_mask = batch[1].to(device)
        labels = batch[2].to(device)

        # Extract embeddings using the pre-trained MentalRoBERTa model
        with torch.no_grad():
            embeddings = model.roberta(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]

        # Forward pass through the classifier
        classifier_outputs = classifier(embeddings)

        # Compute the loss
        loss = criterion(classifier_outputs, labels)

        # Backward pass and optimization
        classifier_optimizer.zero_grad()
        loss.backward()
        classifier_optimizer.step()

        total_loss += loss.item()

        # Calculate training accuracy
        predictions = torch.argmax(classifier_outputs, dim=1)
        total_correct += (predictions == labels).sum().item()
        total_samples += labels.size(0)

    # Calculate and print training accuracy and loss as percentages
    training_accuracy = (total_correct / total_samples) * 100
    training_loss_percentage = (total_loss / len(train_dataloader)) * 100
    print(f'Epoch {epoch + 1}, Classifier Training Loss: {training_loss_percentage:.2f}%, Classifier Training Accuracy: {training_accuracy:.2f}%')

# Evaluate the classifier on the test set
classifier.eval()
test_loss_classifier = 0
test_correct_classifier = 0
test_samples_classifier = 0
all_predictions = []
all_labels = []

for batch in test_dataloader:
    input_ids = batch[0].to(device)
    attention_mask = batch[1].to(device)
    labels = batch[2].to(device)

    # Extract embeddings using the pre-trained MentalRoBERTa model
    with torch.no_grad():
        embeddings = model.roberta(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]

    # Forward pass through the classifier
    classifier_outputs = classifier(embeddings)

    # Compute the loss
    loss = criterion(classifier_outputs, labels)
    test_loss_classifier += loss.item()

    # Calculate test accuracy
    predictions = torch.argmax(classifier_outputs, dim=1)
    test_correct_classifier += (predictions == labels).sum().item()
    test_samples_classifier += labels.size(0)

    # Collect predictions and labels for confusion matrix
    all_predictions.extend(predictions.cpu().numpy())
    all_labels.extend(labels.cpu().numpy())

# Calculate and print test accuracy and loss as percentages for the classifier
test_accuracy_classifier = (test_correct_classifier / test_samples_classifier) * 100
test_loss_percentage_classifier = (test_loss_classifier / len(test_dataloader)) * 100
print(f'Final Classifier Test Loss: {test_loss_percentage_classifier:.2f}%, Final Classifier Test Accuracy: {test_accuracy_classifier:.2f}%')

# Calculate and print accuracy, precision, recall, and F1 score for the classifier
# Convert predictions and labels to numpy arrays
accuracy = accuracy_score(all_labels, all_predictions)
precision = precision_score(all_labels, all_predictions, average='weighted')
recall = recall_score(all_labels, all_predictions, average='weighted')
f1 = f1_score(all_labels, all_predictions, average='weighted')

print(f'Classifier Accuracy: {accuracy:.4f}')
print(f'Classifier Precision: {precision:.4f}')
print(f'Classifier Recall: {recall:.4f}')
print(f'Classifier F1 Score: {f1:.4f}')

# Plot Confusion Matrix
cm = confusion_matrix(all_labels, all_predictions)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()