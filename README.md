# Comparison__Transformers
A controlled 2x2 experiment comparing transformer models (RoBERTa vs. MentalRoBERTa) and preprocessing techniques for mental health text classification. Key finding: Domain-specific pretraining outperforms advanced text cleaning, achieving 84.87% F1-score efficiently.
